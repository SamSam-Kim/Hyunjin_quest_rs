{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1653f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjin0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hjin0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1440,1440) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m largest_object_mask\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# 마스크 생성 및 처리\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m mask_person_or_animal = \u001b[43mget_largest_object_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m mask_blur_processed = cv2.GaussianBlur(mask_person_or_animal, (\u001b[32m31\u001b[39m, \u001b[32m31\u001b[39m), \u001b[32m0\u001b[39m)\n\u001b[32m     53\u001b[39m core_mask_final   = (mask_blur_processed > \u001b[32m150\u001b[39m).astype(np.uint8) * \u001b[32m255\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mget_largest_object_mask\u001b[39m\u001b[34m(image_bgr, class_id)\u001b[39m\n\u001b[32m     29\u001b[39m     output = model(input_tensor)[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     30\u001b[39m     mask = output.argmax(\u001b[32m0\u001b[39m).byte().cpu().numpy()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m object_mask = (\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_id\u001b[49m).astype(np.uint8)\n\u001b[32m     34\u001b[39m labels = measure.label(object_mask)\n\u001b[32m     35\u001b[39m regions = measure.regionprops(labels)\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (1440,1440) (2,) "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# [STEP 0] 이미지 로드\n",
    "img_path = os.path.join(os.getenv('USERPROFILE'), 'Desktop', 'aiffel', 'segmetation', 'images', 'image5.png')\n",
    "img_orig = cv2.imread(img_path)\n",
    "img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# [STEP 1] 가장 큰 객체 마스크 생성 (사람 또는 동물)\n",
    "def get_largest_object_mask(image_bgr, class_id=[17,18]):\n",
    "    model = deeplabv3_resnet101(pretrained=True).to(device).eval()\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = transform(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out'][0]\n",
    "        mask = output.argmax(0).byte().cpu().numpy()\n",
    "\n",
    "    object_mask = (mask == class_id).astype(np.uint8)\n",
    "\n",
    "    labels = measure.label(object_mask)\n",
    "    regions = measure.regionprops(labels)\n",
    "\n",
    "    if not regions:\n",
    "        return np.zeros_like(object_mask)\n",
    "\n",
    "    largest_region = max(regions, key=lambda r: r.area)\n",
    "    largest_object_mask = np.zeros_like(object_mask)\n",
    "    largest_object_mask[labels == largest_region.label] = 1\n",
    "\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    largest_object_mask = cv2.morphologyEx(largest_object_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel) * 255\n",
    "\n",
    "    return largest_object_mask\n",
    "\n",
    "# 마스크 생성 및 처리\n",
    "mask_person_or_animal = get_largest_object_mask(img_orig)\n",
    "mask_blur_processed = cv2.GaussianBlur(mask_person_or_animal, (31, 31), 0)\n",
    "\n",
    "core_mask_final   = (mask_blur_processed > 150).astype(np.uint8) * 255\n",
    "feather_mask_final= ((mask_blur_processed > 5) & (mask_blur_processed <=150)).astype(np.uint8)*255\n",
    "bg_mask_final     = (mask_blur_processed <=5 ).astype(np.uint8)*255\n",
    "\n",
    "# [STEP 1 시각화]\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(core_mask_final, cmap='gray')\n",
    "plt.title(\"Core Mask\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(feather_mask_final, cmap='gray')\n",
    "plt.title(\"Feather Mask\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(bg_mask_final, cmap='gray')\n",
    "plt.title(\"Background Mask\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# [STEP 2] MiDaS로 깊이 맵 생성 및 시각화\n",
    "def estimate_depth_midas(img):\n",
    "    midas_model = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\").to(device).eval()\n",
    "    transform_midas = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
    "\n",
    "    input_img_midas = transform_midas(img).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        depth_prediction = midas_model(input_img_midas)\n",
    "        depth_map_raw = depth_prediction.squeeze().cpu().numpy()\n",
    "\n",
    "        depth_map_filtered = cv2.bilateralFilter(depth_map_raw.astype(np.float32), d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        depth_map_normalized = cv2.normalize(depth_map_filtered, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        return depth_map_normalized\n",
    "\n",
    "depth_map_generated = estimate_depth_midas(img_rgb)\n",
    "\n",
    "# [STEP 2 시각화]\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(depth_map_generated, cmap='plasma')\n",
    "plt.title(\"Depth Map\")\n",
    "plt.axis('off')\n",
    "plt.colorbar(label='Depth')\n",
    "plt.show()\n",
    "\n",
    "# [STEP 3] 블러 및 Feather 적용 + 시각화\n",
    "def apply_blur_with_feathering(img, core_mask, feather_mask, bg_mask):\n",
    "    # 배경 블러 효과 적용\n",
    "    img_blur = cv2.GaussianBlur(img, (61, 61), 0)\n",
    "\n",
    "    # Feather 영역 블렌딩\n",
    "    feather_alpha = feather_mask.astype(np.float32) / 255.0\n",
    "    feather_alpha = feather_alpha[:, :, np.newaxis] * 0.8 + 0.2\n",
    "\n",
    "    # Feather 영역에 블렌딩 적용\n",
    "    feather_blended = (img * feather_alpha + img_blur * (1 - feather_alpha)).astype(np.uint8)\n",
    "\n",
    "    # 최종 이미지 합성\n",
    "    final_img = np.zeros_like(img)\n",
    "    core_3ch = cv2.cvtColor(core_mask, cv2.COLOR_GRAY2BGR)\n",
    "    final_img[core_3ch > 0] = img[core_3ch > 0]\n",
    "    final_img[feather_alpha > 0] = feather_blended[feather_alpha > 0]\n",
    "\n",
    "    return final_img\n",
    "\n",
    "# 블러 및 Feather 적용 결과 생성\n",
    "blurred_img = apply_blur_with_feathering(img_orig, core_mask_final, feather_mask_final, bg_mask_final)\n",
    "\n",
    "# [STEP 3 시각화]\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(blurred_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Blur + Feather Applied\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def replace_background(img, new_bg_path, core_mask, feather_mask):\n",
    "    # 새 배경 이미지 로드 및 크기 조정\n",
    "    new_bg = cv2.imread(new_bg_path)\n",
    "    new_bg_resized = cv2.resize(new_bg, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Feather 영역 블렌딩\n",
    "    feather_alpha = feather_mask.astype(np.float32) / 255.0\n",
    "    feather_alpha = feather_alpha[:, :, np.newaxis]\n",
    "\n",
    "    blended_bg = (img * feather_alpha + new_bg_resized * (1 - feather_alpha)).astype(np.uint8)\n",
    "\n",
    "    # 최종 합성\n",
    "    final_img = np.zeros_like(img)\n",
    "    core_3ch = cv2.cvtColor(core_mask, cv2.COLOR_GRAY2BGR)\n",
    "    final_img[core_3ch > 0] = img[core_3ch > 0]\n",
    "    final_img[core_3ch == 0] = blended_bg[core_3ch == 0]\n",
    "\n",
    "    return final_img\n",
    "\n",
    "# 새 배경 경로 설정\n",
    "new_background_path = os.path.join(os.getenv('USERPROFILE'), 'Desktop', 'bg_image5.jpg')\n",
    "\n",
    "# 기본 배경 교체 결과 생성\n",
    "replaced_bg_img = replace_background(img_orig, new_background_path, core_mask_final, feather_mask_final)\n",
    "\n",
    "# [STEP 4 시각화]\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(replaced_bg_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Background Replaced\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def advanced_background_replace(img, new_bg_path, core_mask, feather_mask, depth_map):\n",
    "    # 새 배경 이미지 로드 및 크기 조정\n",
    "    new_bg_resized = replace_background(img, new_bg_path, core_mask, feather_mask)\n",
    "\n",
    "    # 깊이 맵 기반 그림자 효과 추가\n",
    "    depth_resized = cv2.resize(depth_map, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    shadow_strength = (1 - depth_resized / 255.0)[:, :, np.newaxis] * 0.4\n",
    "    \n",
    "    adjusted_bg_with_shadow = new_bg_resized * (1 - shadow_strength)\n",
    "\n",
    "    return adjusted_bg_with_shadow.astype(np.uint8)\n",
    "\n",
    "# 고급 배경 교체 결과 생성\n",
    "advanced_replaced_img = advanced_background_replace(\n",
    "    img_orig,\n",
    "    new_background_path,\n",
    "    core_mask_final,\n",
    "    feather_mask_final,\n",
    "    depth_map_generated\n",
    ")\n",
    "\n",
    "# [STEP 5 시각화]\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(cv2.cvtColor(replaced_bg_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Basic Background Replacement\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cv2.cvtColor(advanced_replaced_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Advanced Background Replacement\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58296dce",
   "metadata": {},
   "source": [
    "**변환 전 결과물1**\n",
    "-함수적용\n",
    "\n",
    "-![a1.png](./a1.png)\n",
    "\n",
    "![a2.png](./a2.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![a3.png](./a3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7dbf1",
   "metadata": {},
   "source": [
    "**변환 전 결과물2**\n",
    "-함수적용\n",
    "\n",
    "-![a01.png](./a01.png)\n",
    "\n",
    "![a02.png](./a02.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![a03.png](./a03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba09ba",
   "metadata": {},
   "source": [
    "**변환 후 결과물1**\n",
    "-함수적용\n",
    "\n",
    "-![b1.png](./b1.png)\n",
    "\n",
    "![b2.png](./b2.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![b3.png](./b3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667a777",
   "metadata": {},
   "source": [
    "**변환 후 결과물2**\n",
    "-함수적용\n",
    "\n",
    "-![b01.png](./b01.png)\n",
    "\n",
    "![b02.png](./b02.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![b03.png](./b03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d406d70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42509501",
   "metadata": {},
   "source": [
    "포인트 프롬프트 추가\n",
    "\n",
    "고양이 위치에 명시적인 포인트 프롬프트(3개) 추가\n",
    "\n",
    "배경 영역에 음성 프롬프트(3개) 추가하여 고양이와 배경 구분\n",
    "\n",
    "point_labels 배열로 각 프롬프트가 고양이(1)인지 배경(0)인지 구분\n",
    "\n",
    "깊이 맵 활용한 마스크 개선\n",
    "\n",
    "새로운 refine_mask_with_depth() 함수 추가\n",
    "\n",
    "깊이 상위 30%(가까운 물체, threshold_percentile=70)를 고양이로 간주\n",
    "\n",
    "깊이 기반 마스크와 SAM 마스크를 결합해 정확도 향상\n",
    "\n",
    "마스크 후처리 최적화\n",
    "\n",
    "모폴로지 연산(CLOSE와 OPEN) 결합으로 노이즈 제거 및 홀 채우기\n",
    "\n",
    "연결 컴포넌트 분석으로 가장 큰 영역만 선택하여 산발적 오류 제거\n",
    "\n",
    "임계값 조정\n",
    "\n",
    "Core Mask: 150 → 100으로 임계값 낮춤\n",
    "\n",
    "Feather Mask: 25~150 → 10~100으로 범위 조정하여 더 넓은 영역 포함\n",
    "\n",
    "이미지 처리 파이프라인 개선\n",
    "\n",
    "색상 공간 변환 명시적 적용 (RGB/BGR 혼란 방지)\n",
    "\n",
    "마스크 크기와 깊이 맵 크기 일치시키는 부분 강화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114a4b8",
   "metadata": {},
   "source": [
    "많이 바뀌었다다\n",
    "![c1.png](attachment:c1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3906f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1653f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hjin0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hjin0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\hjin0/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "C:\\Users\\hjin0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "C:\\Users\\hjin0\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\timm\\models\\_factory.py:126: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "Using cache found in C:\\Users\\hjin0/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# [STEP 0] 이미지 로드\n",
    "def load_image(image_path):\n",
    "    img_orig = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "    return img_orig, img_rgb\n",
    "\n",
    "# [STEP 1] 가장 큰 객체 마스크 생성 (사람 또는 동물)\n",
    "def get_largest_object_mask(image_bgr, class_id=15):\n",
    "    model = deeplabv3_resnet101(pretrained=True).to(device).eval()\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = transform(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out'][0]\n",
    "        mask = output.argmax(0).byte().cpu().numpy()\n",
    "\n",
    "    object_mask = (mask == class_id).astype(np.uint8)\n",
    "\n",
    "    labels = measure.label(object_mask)\n",
    "    regions = measure.regionprops(labels)\n",
    "\n",
    "    if not regions:\n",
    "        return np.zeros_like(object_mask)\n",
    "\n",
    "    largest_region = max(regions, key=lambda r: r.area)\n",
    "    largest_object_mask = np.zeros_like(object_mask)\n",
    "    largest_object_mask[labels == largest_region.label] = 1\n",
    "\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    largest_object_mask = cv2.morphologyEx(largest_object_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel) * 255\n",
    "\n",
    "    return largest_object_mask\n",
    "\n",
    "# [STEP 2] 깊이 맵 생성\n",
    "def estimate_depth_midas(img):\n",
    "    midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\").to(device).eval()\n",
    "    transform = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n",
    "    img_transformed = transform(img).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(img_transformed)\n",
    "        depth = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "        depth_filtered = cv2.bilateralFilter(depth.astype(np.float32), d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        depth_normalized = cv2.normalize(depth_filtered, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "        return depth_normalized\n",
    "\n",
    "# [STEP 3] 블러 및 Feather 적용\n",
    "def apply_blur_with_feathering(img, core_mask, feather_mask, bg_mask):\n",
    "    img_blur = cv2.GaussianBlur(img, (61, 61), 0)\n",
    "\n",
    "    feather_alpha = feather_mask.astype(np.float32) / 255.0\n",
    "    feather_alpha = feather_alpha[:, :, np.newaxis] * 0.8 + 0.2\n",
    "\n",
    "    feather_blended = (img * feather_alpha + img_blur * (1 - feather_alpha)).astype(np.uint8)\n",
    "\n",
    "    final_img = np.zeros_like(img)\n",
    "    core_3ch = cv2.cvtColor(core_mask, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    final_img[core_3ch > 0] = img[core_3ch > 0]\n",
    "    final_img[feather_alpha > 0] = feather_blended[feather_alpha > 0]\n",
    "\n",
    "    return final_img\n",
    "\n",
    "# [STEP 4] 기본 배경 교체\n",
    "def replace_background(img, new_bg_path, core_mask, feather_mask):\n",
    "    new_bg = cv2.imread(new_bg_path)\n",
    "    new_bg_resized = cv2.resize(new_bg, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    feather_alpha = feather_mask.astype(np.float32) / 255.0\n",
    "    feather_alpha = feather_alpha[:, :, np.newaxis]\n",
    "\n",
    "    blended_bg = (img * feather_alpha + new_bg_resized * (1 - feather_alpha)).astype(np.uint8)\n",
    "\n",
    "    final_img = np.zeros_like(img)\n",
    "    core_3ch = cv2.cvtColor(core_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    final_img[core_3ch > 0] = img[core_3ch > 0]\n",
    "    final_img[core_3ch == 0] = blended_bg[core_3ch == 0]\n",
    "\n",
    "    return final_img\n",
    "\n",
    "# 실행 및 시각화 예제\n",
    "if __name__ == \"__main__\":\n",
    "    # 이미지 경로 설정\n",
    "    image_path = os.path.join(os.getenv('USERPROFILE'), 'Desktop', 'image.png')\n",
    "    \n",
    "    # 배경 이미지 경로 설정\n",
    "    new_background_path = os.path.join(os.getenv('USERPROFILE'), 'Desktop', 'background.jpg')\n",
    "    \n",
    "    # 이미지 로드 및 처리 단계 실행\n",
    "    img_orig, img_rgb = load_image(image_path)\n",
    "    \n",
    "    mask_person_or_animal = get_largest_object_mask(img_orig) # 사람(class_id=15) 또는 동물(class_id 변경 가능)\n",
    "    \n",
    "    depth_map_generated = estimate_depth_midas(img_rgb)\n",
    "    \n",
    "    # Core/Feather/Background 분리 마스크 생성\n",
    "    mask_blur_processed = cv2.GaussianBlur(mask_person_or_animal, (31, 31), 0)\n",
    "    \n",
    "    core_mask_final   = (mask_blur_processed >150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58296dce",
   "metadata": {},
   "source": [
    "**변환 전 결과물1**\n",
    "-함수적용\n",
    "\n",
    "-![a1.png](./a1.png)\n",
    "\n",
    "![a2.png](./a2.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![a3.png](./a3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7dbf1",
   "metadata": {},
   "source": [
    "**변환 전 결과물2**\n",
    "-함수적용\n",
    "\n",
    "-![a01.png](./a01.png)\n",
    "\n",
    "![a02.png](./a02.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![a03.png](./a03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba09ba",
   "metadata": {},
   "source": [
    "**변환 후 결과물1**\n",
    "-함수적용\n",
    "\n",
    "-![b1.png](./b1.png)\n",
    "\n",
    "![b2.png](./b2.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![b3.png](./b3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667a777",
   "metadata": {},
   "source": [
    "**변환 후 결과물2**\n",
    "-함수적용\n",
    "\n",
    "-![b01.png](./b01.png)\n",
    "\n",
    "![b02.png](./b02.png)\n",
    "\n",
    "-적용 후 이미지\n",
    "![b03.png](./b03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d406d70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20f3906f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
